format_data方法解读：
e3_behavior_v1.txt文件记录了每个用户每次推荐的item以及用户在该次推荐的item中点击的某些item，相当于是记录了推荐
    引擎每次给用户推荐的items以及记录该次推荐时，用户对哪些item做了点击。
e3_news_category_v1.txt文件记录了每个item对应的category id，类似一条新闻有可能属于多个类别，
e3_user_news_feature_v1.txt文件记录了某个用户对某个item所提取到的特征。根据userid+itemid即可得到该用户对该item的
    特征

data_click记录第n次点击所对应的item处在所有展示的item的第几个位置。
feature_click[user]=np.zeros([click_t,20])，click_t记录了用户点击过的item个数，因为在推荐系统里，在排序的时候
    是要过滤已经推荐过的item的，所有理论上，用户不存在重复点击某个item的可能性，但是反过来，在一段时间内推荐理论上是
    不存在重复推荐的可能性，如果时间窗口很长，是存在重复推荐的可能性的，为了方便理解代码，可认为不存在重复点击的可能性。
    所以，click_t即代表用户总共点击了多少个item。每个item20维特征表示。
news_dict代表在所有展示的item中，各个item在该用户展示时的索引。比如news_dict[1]=3,代表id为1的item在该用户
    的所有展示中处于第3个位置
feature记录了每个用户所有展示的item的特征，
    即feature_click记录了用户点击的item的特征，feature记录用户所有展示的item的特征

第一步:调用format_data方法
第二步:调用construct_placeholder方法，定义了一系列的占位符
第三步:调用construct_model方法，在该方法里调用了construct_graph方法
    construct_graph方法:


第四步:data_perform方法:
    遍历的是8个桶里面的所有用户，构造数据，disp_feature是二维数组，把当前set中用户的disp feature拼接到一起，
    click_feature一样，也是把当前set的用户的click feature拼接到一起。具体看代码。

然后调用construct_model方法

训练集的数据是在一个时间周期内(比如一个月)。把每个用户的展示按时间排序，构成一个长长的展示列表

环境的使用:
在真实环境中使用时，应该是拿用户直接跟模型学到的环境交互。然后用交互的数据学习一个策略。而本代码采用的是offline的方式，假设目标策略是随机策略，
    用随机策略产生曝光的数据，然后根据环境得到用户可能的选择，得到(s,a),有了训练集后，就可以根据论文中的方法学到一个PI，理论上，这个学到的PI应该和
    随机策略很像，因为训练集是随机策略产生的。

环境的使用:
1. 利用随机策略与环境交互得到一系列训练集 100个time_horizon
2. 计算该用户的reward得到y，同时，计算该用户对此次曝光的item中各个item的概率。
3. 根据计算的概率预测用户对该次曝光的item的点击情况并更新state，即得到了s,a,r,s_,r为每个用户对当前10个item的reward的加权和
    [best_action_reward, transition_p] = sess.run([Reward_r, trans_p], feed_dict=reward_feed_dict)


策略PI的训练
1. 拿到从环境收集到的数据data_collection 100 time_horizon,虽然有100个time_horizon，但是此时是不用区分的，因为t时刻已经包含了前面时刻的信息了
2. 每个k利用环境的reward与当前计算出来的reward使用squared_difference训练
    loss_k[ii] = tf.reduce_mean(tf.squared_difference(q_value_k[ii], y_label))#y_label就是env算出来的reward
    opt_k[ii] = tf.train.AdamOptimizer(learning_rate=_lr)
    注意:训练的时候拟合的是用户的reward,只需要让每个用户拟合各自的reward即可。
训练的时候使用的是1000个用户，100个time_horizon,当前s状态下执行动作action后得到reward，由于是交互，得到reward后会返回一个新的状态s'
直到迭代100个time stamp。回想一下，平时在使用gym的env环境时也是这个流程，不停的与环境交互得到r和s'，然后将数据放入replay_buffer

TEST
3. 利用cascade dqn方式得到最优action
    max_action_k[ii] = tf.argmax(q1_tensor, axis=1)
4. 得到最优的action，即向用户曝光的action后，
    best_action = sess.run([max_action, max_action_disp_features], feed_dict=max_q_feed_dict)
    根据最优的action计算没一个item的reward并预测用户对该10个item的点击情况
    [reward_u, transition_p] = sess.run([u_disp, trans_p], feed_dict=reward_feed_dict)
5. 重新计算用户的state，并过滤一些已经不满足要求的用户,得到state_
    sample_new_states_for_train
6. 根据新的state即state_计算reward
    r+gamma*Q(s')
    y_value = sampled_reward + _gamma * max_q_val
7. 根据mse来训练策略pi
测试时，一般是一个用户一个用户测试，其实也可以是多个用户一起来测试，比如100个用户，但是不能再像训练前收集数据那样一下子执行100个time stamp了，
    我们要根据s,a得到reward和s',如果和train一样放到data_collection,虽然reward知道，但是s'却不知道了。

为什么不用全局id了，猜想是因为如果总量SKU数量太大的话，argmax的计算量就会很大。但是如果只计算用户相关的sku，计算量就会少很多
argmax只计算当前batch中与用户相关的最大的action的数量
环境的训练时通过gan的方式来训练的，策略的训练时通过减少和reward的平方的差异为目标来优化的。

根据DQN的训练思路，环境会根据当前的state执行从agent中选出的action，转换到新的状态state'，并返回reward，


