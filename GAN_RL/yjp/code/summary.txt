1. 训练阶段，在与环境交互收集数据的过程中，只需要(s,a,r)即可.r+gamma*r'
    sample_new_states_for_train中并不会出现重复的用户，因此只需要sampled_reward=[]即可。

2. 在test时，需要让用户经历完整的time步，比如time_horizon=6，我们要算用户的平均reward，因此sim_u_reward一个用户
    会有time个数

RL的训练中， 有2个for循环，一个针对episode的外层循环，一个针对max len的内层循环。在推荐系统领域，用户应该就是对应的episode，每个用户与环境交互的数据
    应该就是max len。还可以把所有用户当成一个用户看待，自定义episode，在每个episode下进行max len的循环操作。
    在常规的RL训练时，在与环境交互时，只有主线程与环境交互，其实相当于只有一个人与环境交互。如果多个线程与环境交互就会得到更多的数据。
    类比到强化学习的推荐系统来说，用户量既可以看作是外层的episode的循环次数，也可以看作是一个用户，将数据拼接即可

算法的迭代是针对训练集进行epochs的迭代。相当于每条数据进行了epochs次数的循环。gan_rl.py与gan_rl2.py的区别在于，gan_rl.py的外层循环是epoch，每个epoch
    对所有的训练用户进行循环。在处理数据时相当于是每个用户循环了epoch次数的生成训练集。而由于LSTM模式生成训练集很耗时间。LSTM训练节约的时间抵不上处理数据所消耗的时间，
    这就是为什么明明LSTM比PW方式快却耗时更长的原因，大部分时间都花在生成训练集上了。相反，gan_rl2.py，外层针对的循环是用户，虽然还是每个用户会训练epoch次，但是每个用户
    生成训练数据的次数是1次，而不是gan_rl.py中的epochs次。
    gan_rl.py中一个epoch内的数据是不一样的。而gan_rl2.py中，对于同一batch用户，拿该用户下的数据直接进行epoch次数的重复训练再用下一个batch的用户训练
    前者是一个epoch跑完所有的用户，前后2次训练的数据是不一样的。后者时一个batch内的用户先跑完所有的epoch，再用下一个batch的用户训练，前后2次训练的数据有时候是一样的。真正意义
    上的重复训练。类别记单词。假设总共10个单词，每个单词要重复10次。前者是把所有的单词过一遍，然后重复所有单词十遍，后者是依次将每个单词重复十遍。

通过对比分析，证明pw和lstm的效果差不多，pw比lstm稍微好点，但速度上来说，如果采用gan_rl2.py的方式，lstm比pw快很多
通过对比分析可知gan_rl2.py的方式和gan_rl.py差不多，但前者震荡的比较厉害
结论:最好还是每一个epoch遍历一遍所有的数据，而不是每个batch的用户一次性遍历完所有的epoch。如果第一种方式耗时时间实在太长，则可以用第二种(batch 遍历完所有的epoch),但epoch的
    次数不宜太多，经过验证epoch=20的效果gan_rl.py和gan_rl2.py效果差不多，epoch=50时，gan_rl2.py有点过拟合的味道。相当于是每个batch的用户先一次性迭代50次有点多.

jpg1目录存放的迭代50次的效果，jpg2存放的是20次的效果，jpg3存放的是pw模型修改后并且过滤没有点击数据的用户后的20次的效果
jpg4存放的是仅仅过滤没有发生点击数据的用户且pw生成数据的方式使用的是最原始的方式
jpg5存放的是仅仅过滤没有发生点击数据的用户且pw生成数据的方式使用的是最原始的方式 num_iters=50
jpg6是删除clip_by_value代码后的方式
jpg7是删除clip_by_value后，只运行gan_rl.py的方式，最终只要gan_rl.py的方式,num_iters = 50
pw生产数据三角矩阵的方式改动之后，不管是loss，p1，还是p2的曲线都不正常。应该是最开始的生成数据的方式是正确的。

过滤到数据后，50次迭代的效果也很好。
不管是pw还是lstm方式，使用什么模型，影响不大。主要是数据的迭代方式。如果前后2次迭代所使用的数据是相同的，如gan_rl2.py中，虽然总体来说loss是下降的，下降过程不太平稳。
    相比来说，gan_rl.py的方式，先epoch再batch的方式要平稳很多。

gan_rl2.py pw 方式，num_iters=50 会出现loss为nan的情况，因为u_disp太大的话，exp后数量级太大，

不知道为啥，main.py中的效果比main_.py中的效果要好。
main.py中迭代10450次 100%|██████████| 10/10 [4:22:54<00:00, 1577.47s/it] finished!!!!!!,time cost:284.424596508344 m loss下降的效果不错
main_py中10790次   100%|██████████| 74/74 [4:18:15<00:00, 209.40s/it]  finished!!!!!!,time cost:275.25697528918585 m loss下降的效果很差
loss_random_batch.out 训练集的用户采用循环的方式来生成训练集，而不是一次性处理来生成训练集,想验证一下是不是一次性生成数据时耗时比较长
    100%|██████████| 10/10 [4:36:54<00:00, 1661.42s/it] 相差时间不大，并不能说明什么，和当时的计算机计算资源有关。效果和main.py一样，
    理论上是使用循环来生成训练集还是一次性生成训练集对loss应该也不会有影响。
main_greedy_max.out 和main_greedy_max_.out是求max_value的时候，使用的是当前k个item最大的value，而并非是最后一个item最大的value
main_greedy_max.out 100%|██████████| 10/10 [5:18:34<00:00, 1911.46s/it]  finished!!!!!!,time cost:403.33112662235897 m
main_greedy_max_.out 100%|██████████| 74/74 [7:02:29<00:00, 342.56s/it] finished!!!!!!,time cost:431.56159255107247 m
    效果好像没有用最后一次item推荐的最大值好。

心得:训练的时候还得一个epoch遍历所有的数据，而不能采用一个单词先记20遍再记其他单词的思路，
epoch=5最好

用groupby比用filter快些